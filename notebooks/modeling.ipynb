{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbf4dc17",
   "metadata": {},
   "source": [
    "# Fraud Detection Modeling\n",
    "\n",
    "This notebook contains functions for modeling fraud detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "473540a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original directory: /Users/adrianhajdukiewicz/projects/private/recruitment_2025_data_biz_hackathon/notebooks\n",
      "Changed directory to: /Users/adrianhajdukiewicz/projects/private/recruitment_2025_data_biz_hackathon\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Users/adrianhajdukiewicz/projects/private/recruitment_2025_data_biz_hackathon'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "import optuna\n",
    "from typing import Dict, List, Tuple, Optional, Any, Union\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score, \n",
    "                             precision_recall_curve, f1_score, roc_curve)\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set matplotlib style\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Configure display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 200)\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "# Import the utility function for one-time directory change\n",
    "from src.utilities.path_utilities import ensure_parent_dir\n",
    "\n",
    "# Change to parent directory (only happens once even if cell is re-executed)\n",
    "ensure_parent_dir('notebook_setup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e16c9f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.read_csv('data/df_merged.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa111273",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a49492f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_categorical_features(df: pd.DataFrame, \n",
    "                               categorical_cols: Optional[List[str]] = None) -> Tuple[pd.DataFrame, Dict[str, LabelEncoder]]:\n",
    "    \"\"\"Encode categorical features using LabelEncoder.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing the features\n",
    "        categorical_cols: List of categorical columns to encode. If None, defaults to a predefined list\n",
    "        \n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "            - DataFrame with encoded features\n",
    "            - Dictionary mapping column names to their corresponding LabelEncoder objects\n",
    "    \"\"\"\n",
    "    if categorical_cols is None:\n",
    "        categorical_cols = ['user_id', 'merchant_id', 'channel', 'currency', 'device', 'payment_method',\n",
    "                           'education', 'primary_source_of_income', 'user_country', 'category',\n",
    "                           'merchant_country', 'is_first_time_merchant']\n",
    "        \n",
    "        # Add any additional categorical columns extracted from location\n",
    "        if 'country' in df.columns:\n",
    "            categorical_cols.append('country')\n",
    "        if 'city' in df.columns:\n",
    "            categorical_cols.append('city')\n",
    "        if 'administrative_region_1' in df.columns:\n",
    "            categorical_cols.append('administrative_region_1')\n",
    "    \n",
    "    # Apply label encoding\n",
    "    label_encoders = {}\n",
    "    for col in categorical_cols:\n",
    "        if col in df.columns:\n",
    "            le = LabelEncoder()\n",
    "            df[col + '_encoded'] = le.fit_transform(df[col].astype(str))\n",
    "            label_encoders[col] = le\n",
    "    \n",
    "    return df, label_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d592fff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_modeling_dataset(df: pd.DataFrame, \n",
    "                            cols_to_drop: Optional[List[str]] = None,\n",
    "                            categorical_cols: Optional[List[str]] = None) -> pd.DataFrame:\n",
    "    \"\"\"Prepare the dataset for modeling by dropping unnecessary columns.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to prepare\n",
    "        cols_to_drop: List of columns to drop. If None, uses default list\n",
    "        categorical_cols: List of categorical columns for reference\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame ready for modeling\n",
    "    \"\"\"\n",
    "    # Define default columns to drop if not provided\n",
    "    if cols_to_drop is None:\n",
    "        cols_to_drop = ['transaction_id', 'user_id', 'merchant_id', 'timestamp', 'location', \n",
    "                        'prev_timestamp', 'signup_date', 'date']\n",
    "    \n",
    "    # If categorical_cols is provided, extend cols_to_drop with original categorical columns\n",
    "    if categorical_cols is not None:\n",
    "        cols_to_drop.extend([col for col in categorical_cols if col in df.columns])\n",
    "    \n",
    "    # Drop specified columns that exist in the dataframe\n",
    "    modeling_df = df.drop(columns=[col for col in cols_to_drop if col in df.columns])\n",
    "    \n",
    "    return modeling_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c44b76e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset_for_modeling(df: pd.DataFrame, \n",
    "                              date_cols: Optional[List[str]] = None) -> pd.DataFrame:\n",
    "    \"\"\"Clean the dataset by handling date columns, dict columns, and encoding categorical features.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to clean\n",
    "        date_cols: List of date/time columns to drop. If None, uses default list\n",
    "        \n",
    "    Returns:\n",
    "        Cleaned DataFrame ready for modeling\n",
    "    \"\"\"\n",
    "    modeling_df = df.copy()\n",
    "    \n",
    "    # Define date columns to drop if not provided\n",
    "    if date_cols is None:\n",
    "        date_cols = ['timestamp', 'date', 'signup_date', 'weekday_name']\n",
    "    \n",
    "    # 1. Drop date/time columns\n",
    "    for col in date_cols:\n",
    "        if col in modeling_df.columns:\n",
    "            modeling_df = modeling_df.drop(col, axis=1)\n",
    "            print(f\"Dropped date/time column: {col}\")\n",
    "    \n",
    "    # 2. Drop columns that contain dicts (e.g., 'location')\n",
    "    for col in modeling_df.columns:\n",
    "        if modeling_df[col].apply(lambda x: isinstance(x, dict)).any():\n",
    "            modeling_df = modeling_df.drop(col, axis=1)\n",
    "            print(f\"Dropped dict column: {col}\")\n",
    "    \n",
    "    # 3. Encode all object (string) columns to categorical codes\n",
    "    for col in modeling_df.select_dtypes(include=['object']).columns:\n",
    "        modeling_df[col] = modeling_df[col].astype('category').cat.codes\n",
    "        print(f\"Encoded categorical column: {col}\")\n",
    "    \n",
    "    return modeling_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49d45ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(df: pd.DataFrame, \n",
    "                 target_col: str = 'is_fraud', \n",
    "                 test_size: float = 0.2, \n",
    "                 random_state: int = 42) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:\n",
    "    \"\"\"Split the dataset into features and target, and then into train and test sets.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to split\n",
    "        target_col: Name of the target column\n",
    "        test_size: Proportion of the dataset to include in the test split\n",
    "        random_state: Random state for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        Tuple containing X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    # Define features and target\n",
    "    X = df.drop(target_col, axis=1)\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "    print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "    print(f\"Fraud samples in training set: {y_train.sum()}\")\n",
    "    print(f\"Fraud samples in test set: {y_test.sum()}\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e787d0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_smote(X_train: pd.DataFrame, \n",
    "               y_train: pd.Series, \n",
    "               random_state: int = 42,\n",
    "               apply: bool = False) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    \"\"\"Apply SMOTE to handle class imbalance.\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training features\n",
    "        y_train: Training target\n",
    "        random_state: Random state for reproducibility\n",
    "        apply: Whether to actually apply SMOTE or return original data\n",
    "        \n",
    "    Returns:\n",
    "        Tuple containing resampled X_train and y_train\n",
    "    \"\"\"\n",
    "    if apply:\n",
    "        smote = SMOTE(random_state=random_state)\n",
    "        X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "        \n",
    "        print(f\"Original training set shape: {X_train.shape}\")\n",
    "        print(f\"Resampled training set shape: {X_train_resampled.shape}\")\n",
    "        print(f\"Fraud samples in original training set: {y_train.sum()}\")\n",
    "        print(f\"Fraud samples in resampled training set: {y_train_resampled.sum()}\")\n",
    "    else:\n",
    "        X_train_resampled = X_train\n",
    "        y_train_resampled = y_train\n",
    "        print(\"SMOTE not applied. Using original training data.\")\n",
    "    \n",
    "    return X_train_resampled, y_train_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffd3123",
   "metadata": {},
   "source": [
    "## 2. Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff72bb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optuna_objective(X_train: pd.DataFrame, \n",
    "                           y_train: pd.Series,\n",
    "                           pos_weight: float = None,\n",
    "                           n_splits: int = 5) -> callable:\n",
    "    \"\"\"Create an objective function for Optuna hyperparameter optimization.\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training features\n",
    "        y_train: Training target\n",
    "        pos_weight: Weight of positive class for imbalanced data\n",
    "        n_splits: Number of cross-validation splits\n",
    "        \n",
    "    Returns:\n",
    "        Objective function to be used in Optuna study\n",
    "    \"\"\"\n",
    "    # Calculate default pos_weight if not provided\n",
    "    if pos_weight is None:\n",
    "        n_neg = len(y_train) - sum(y_train)\n",
    "        n_pos = sum(y_train)\n",
    "        pos_weight = n_neg / n_pos if n_pos > 0 else 1.0\n",
    "    \n",
    "    def objective(trial):\n",
    "        # Define hyperparameters to optimize with well-tested ranges\n",
    "        param = {\n",
    "            'objective': 'binary:logistic',\n",
    "            'eval_metric': 'auc',\n",
    "            'booster': 'gbtree',\n",
    "            'lambda': trial.suggest_float('lambda', 1e-3, 5.0, log=True),\n",
    "            'alpha': trial.suggest_float('alpha', 1e-3, 5.0, log=True),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 40, 50),\n",
    "            'max_depth': trial.suggest_int('max_depth', 4, 8),\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 5),\n",
    "            'gamma': trial.suggest_float('gamma', 1e-8, 0.5, log=True),\n",
    "            'subsample': trial.suggest_float('subsample', 0.7, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1.0),\n",
    "            'scale_pos_weight': pos_weight,\n",
    "            'random_state': 42\n",
    "        }\n",
    "        \n",
    "        # Initialize cross-validation\n",
    "        cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        cv_scores = []\n",
    "        \n",
    "        # Perform cross-validation\n",
    "        for train_idx, val_idx in cv.split(X_train, y_train):\n",
    "            X_train_fold = X_train.iloc[train_idx]\n",
    "            y_train_fold = y_train.iloc[train_idx]\n",
    "            X_val_fold = X_train.iloc[val_idx]\n",
    "            y_val_fold = y_train.iloc[val_idx]\n",
    "            \n",
    "            # Train model\n",
    "            model = xgb.XGBClassifier(**param)\n",
    "            model.fit(\n",
    "                X_train_fold, y_train_fold,\n",
    "                eval_set=[(X_val_fold, y_val_fold)],\n",
    "                verbose=False\n",
    "            )\n",
    "            \n",
    "            # Make predictions and calculate F1 score\n",
    "            y_pred = model.predict(X_val_fold)\n",
    "            cv_scores.append(f1_score(y_val_fold, y_pred))\n",
    "        \n",
    "        # Return mean CV score\n",
    "        return np.mean(cv_scores)\n",
    "    \n",
    "    return objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67209d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_hyperparameter_optimization(X_train: pd.DataFrame, \n",
    "                                   y_train: pd.Series,\n",
    "                                   n_trials: int = 200,\n",
    "                                   pos_weight: float = None,\n",
    "                                   n_splits: int = 5,\n",
    "                                   plot_results: bool = True) -> Dict:\n",
    "    \"\"\"Run hyperparameter optimization using Optuna.\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training features\n",
    "        y_train: Training target\n",
    "        n_trials: Number of optimization trials\n",
    "        pos_weight: Weight of positive class for imbalanced data\n",
    "        n_splits: Number of cross-validation splits\n",
    "        plot_results: Whether to plot optimization results\n",
    "        \n",
    "    Returns:\n",
    "        Dict containing best parameters and study object\n",
    "    \"\"\"\n",
    "    # Create objective function\n",
    "    objective = create_optuna_objective(X_train, y_train, pos_weight, n_splits)\n",
    "    \n",
    "    # Create study\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    \n",
    "    # Run optimization\n",
    "    print(\"Starting optimization...\")\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    \n",
    "    # Print best parameters and score\n",
    "    print(\"\\nBest parameters:\", study.best_params)\n",
    "    print(\"Best F1 score:\", study.best_value)\n",
    "    \n",
    "    # Analyze parameter importance\n",
    "    importance = optuna.importance.get_param_importances(study)\n",
    "    print(\"\\nParameter Importance:\")\n",
    "    for param, score in sorted(importance.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"{param}: {score:.4f}\")\n",
    "    \n",
    "    if plot_results:\n",
    "        # Plot parameter importance\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        importance_df = pd.DataFrame({\n",
    "            'Parameter': list(importance.keys()),\n",
    "            'Importance': list(importance.values())\n",
    "        })\n",
    "        importance_df = importance_df.sort_values('Importance', ascending=True)\n",
    "        plt.barh(importance_df['Parameter'], importance_df['Importance'])\n",
    "        plt.title('Parameter Importance')\n",
    "        plt.xlabel('Importance Score')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return {'best_params': study.best_params, 'study': study}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27137e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_param_distributions(study: optuna.study.Study, figsize: Tuple[int, int] = (15, 15)) -> None:\n",
    "    \"\"\"Plot parameter distributions from an Optuna study.\n",
    "    \n",
    "    Args:\n",
    "        study: The Optuna study object\n",
    "        figsize: Size of the figure (width, height)\n",
    "    \"\"\"\n",
    "    trials_df = study.trials_dataframe()\n",
    "    param_names = [col for col in trials_df.columns if col.startswith('params_')]\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 3, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, param_name in enumerate(param_names):\n",
    "        if idx < len(axes):\n",
    "            ax = axes[idx]\n",
    "            param_name_clean = param_name[7:]  # Remove 'params_' prefix\n",
    "            ax.scatter(trials_df[param_name], trials_df['value'])\n",
    "            ax.set_xlabel(param_name_clean)\n",
    "            ax.set_ylabel('F1 Score')\n",
    "            ax.set_title(f'{param_name_clean} vs F1 Score')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3991dc4e",
   "metadata": {},
   "source": [
    "## 3. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30307b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgb_model(X_train: pd.DataFrame, \n",
    "                   y_train: pd.Series,\n",
    "                   X_test: pd.DataFrame = None,\n",
    "                   y_test: pd.Series = None,\n",
    "                   params: Dict = None,\n",
    "                   verbose: int = 100) -> xgb.XGBClassifier:\n",
    "    \"\"\"Train an XGBoost model with the given parameters.\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training features\n",
    "        y_train: Training target\n",
    "        X_test: Test features for evaluation during training\n",
    "        y_test: Test target for evaluation during training\n",
    "        params: Dictionary of model parameters\n",
    "        verbose: Verbosity of training output\n",
    "        \n",
    "    Returns:\n",
    "        Trained XGBClassifier model\n",
    "    \"\"\"\n",
    "    # Set default parameters if not provided\n",
    "    if params is None:\n",
    "        n_neg = len(y_train) - sum(y_train)\n",
    "        n_pos = sum(y_train)\n",
    "        scale_pos_weight = n_neg / n_pos if n_pos > 0 else 1.0\n",
    "        \n",
    "        params = {\n",
    "            'objective': 'binary:logistic',\n",
    "            'n_estimators': 40,\n",
    "            'learning_rate': 0.1,\n",
    "            'max_depth': 5,\n",
    "            'min_child_weight': 1,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'gamma': 0,\n",
    "            'scale_pos_weight': scale_pos_weight,\n",
    "            'random_state': 42,\n",
    "            'eval_metric': 'auc'\n",
    "        }\n",
    "    \n",
    "    # Convert categorical variables to codes\n",
    "    X_train = X_train.apply(lambda x: x.cat.codes if x.dtype.name == 'category' else x)\n",
    "    if X_test is not None:\n",
    "        X_test = X_test.apply(lambda x: x.cat.codes if x.dtype.name == 'category' else x)\n",
    "    \n",
    "    # Initialize and train model\n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    \n",
    "    # Set up evaluation sets\n",
    "    eval_sets = [(X_train, y_train)]\n",
    "    if X_test is not None and y_test is not None:\n",
    "        eval_sets.append((X_test, y_test))\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=eval_sets,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4f40f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(model: xgb.XGBClassifier, \n",
    "                           max_features: int = 20,\n",
    "                           importance_type: str = 'gain',\n",
    "                           figsize: Tuple[int, int] = (12, 8)) -> pd.DataFrame:\n",
    "    \"\"\"Plot and return feature importance from a trained XGBoost model.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained XGBClassifier model\n",
    "        max_features: Maximum number of features to display\n",
    "        importance_type: Type of feature importance ('gain', 'weight', 'cover', 'total_gain', 'total_cover')\n",
    "        figsize: Size of the figure (width, height)\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with feature importance scores\n",
    "    \"\"\"\n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=figsize)\n",
    "    xgb.plot_importance(model, max_num_features=max_features, importance_type=importance_type)\n",
    "    plt.title('Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Get feature importance scores\n",
    "    feature_importance = model.get_booster().get_score(importance_type=importance_type)\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': list(feature_importance.keys()),\n",
    "        'Importance': list(feature_importance.values())\n",
    "    }).sort_values('Importance', ascending=False).head(max_features)\n",
    "    \n",
    "    return importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b283545b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model: xgb.XGBClassifier, \n",
    "                  X_test: pd.DataFrame, \n",
    "                  y_test: pd.Series) -> Dict[str, float]:\n",
    "    \"\"\"Evaluate a trained model and return performance metrics.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained classification model\n",
    "        X_test: Test features\n",
    "        y_test: Test target\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing performance metrics\n",
    "    \"\"\"\n",
    "    # Convert categorical variables to codes if needed\n",
    "    X_test = X_test.apply(lambda x: x.cat.codes if x.dtype.name == 'category' else x)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Print evaluation results\n",
    "    print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'roc_auc': roc_auc,\n",
    "        'f1_score': f1,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f258f7c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Plot ROC curve\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m roc_curve\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m fpr, tpr, thresholds = roc_curve(\u001b[43my_test\u001b[49m, y_pred_proba)\n\u001b[32m      5\u001b[39m plt.figure(figsize=(\u001b[32m10\u001b[39m, \u001b[32m6\u001b[39m))\n\u001b[32m      6\u001b[39m plt.plot(fpr, tpr, label=\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mAUC = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mroc_auc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot ROC curve\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.4f}')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff979423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.51      0.66     91518\n",
      "           1       0.10      0.60      0.17      8482\n",
      "\n",
      "    accuracy                           0.52    100000\n",
      "   macro avg       0.52      0.55      0.42    100000\n",
      "weighted avg       0.86      0.52      0.62    100000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_classification_report(y_test: pd.Series, y_pred: np.ndarray) -> None:\n",
    "    \"\"\"Print classification report with precision, recall, and F1-score.\n",
    "    \n",
    "    Args:\n",
    "        y_test: True target values\n",
    "        y_pred: Predicted target values\n",
    "    \"\"\"\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7251690d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_confusion_matrix(y_test: pd.Series, \n",
    "                         y_pred: np.ndarray, \n",
    "                         figsize: Tuple[int, int] = (8, 6)) -> None:\n",
    "    \"\"\"Plot confusion matrix to visualize model performance.\n",
    "    \n",
    "    Args:\n",
    "        y_test: True target values\n",
    "        y_pred: Predicted target values\n",
    "        figsize: Size of the figure (width, height)\n",
    "    \"\"\"\n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ac49b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b2f66a5",
   "metadata": {},
   "source": [
    "## 5. Example Usage\n",
    "\n",
    "Below is an example of how to use the functions defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3148b963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of the pipeline\n",
    "# Uncomment the code below to run the full pipeline\n",
    "\n",
    "# Preprocess data\n",
    "df_merged, label_encoders = encode_categorical_features(df_merged)\n",
    "modeling_df = prepare_modeling_dataset(df_merged)\n",
    "modeling_df = clean_dataset_for_modeling(modeling_df)\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = split_dataset(modeling_df)\n",
    "\n",
    "# Handle class imbalance\n",
    "X_train_resampled, y_train_resampled = apply_smote(X_train, y_train, apply=False)\n",
    "\n",
    "# Train model with default parameters\n",
    "model = train_xgb_model(X_train_resampled, y_train_resampled, X_test, y_test)\n",
    "\n",
    "# Evaluate model\n",
    "metrics = evaluate_model(model, X_test, y_test)\n",
    "\n",
    "# Visualize results\n",
    "importance_df = plot_feature_importance(model)\n",
    "plot_roc_curve(y_test, metrics['y_pred_proba'])\n",
    "print_classification_report(y_test, metrics['y_pred'])\n",
    "plot_confusion_matrix(y_test, metrics['y_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c36caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Alternatively, use the complete pipeline with a single function\n",
    "# results = run_fraud_detection_pipeline(\n",
    "#     df_merged, \n",
    "#     optimize_hyperparameters=True, \n",
    "#     apply_smote_resampling=False,\n",
    "#     n_trials=50\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
